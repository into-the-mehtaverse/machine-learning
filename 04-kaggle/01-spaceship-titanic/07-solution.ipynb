{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7097cd6f-cb7f-4ff3-8037-7280db56b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "sample_submission = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca89b64b-164e-47d3-b66d-7df489a6d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add TotalSpend numeric feature and NoSpend numeric feature\n",
    "\n",
    "for df in (df_train, df_test):\n",
    "    # Sum all the spending columns\n",
    "    df[\"TotalSpend\"] = (\n",
    "        df[\"RoomService\"]\n",
    "        + df[\"FoodCourt\"]\n",
    "        + df[\"ShoppingMall\"]\n",
    "        + df[\"Spa\"]\n",
    "        + df[\"VRDeck\"]\n",
    "    )\n",
    "\n",
    "    # Optional: flag for passengers who spent nothing\n",
    "    df[\"NoSpend\"] = (df[\"TotalSpend\"] == 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0a396b-230c-42d3-a454-cd63d1c263f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split cabin feature into three, for deck, floor and side\n",
    "\n",
    "for df in (df_train, df_test):\n",
    "    cabin_split = df[\"Cabin\"].str.split(\"/\", expand=True)\n",
    "\n",
    "    df[\"CabinDeck\"] = cabin_split[0]                      # e.g. \"B\"\n",
    "    df[\"CabinNum\"] = pd.to_numeric(cabin_split[1], errors=\"coerce\")  # e.g. 45\n",
    "    df[\"CabinSide\"] = cabin_split[2]                      # e.g. \"P\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d973475-f2a0-4719-af82-0e6785aacc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## exploit groups\n",
    "\n",
    "for df in (df_train, df_test):\n",
    "    df[\"GroupId\"] = df[\"PassengerId\"].str.split(\"_\").str[0]\n",
    "    group_sizes = df.groupby(\"GroupId\")[\"GroupId\"].transform(\"count\")\n",
    "    df[\"GroupSize\"] = group_sizes\n",
    "    df[\"IsAlone\"] = (df[\"GroupSize\"] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36baa688-9160-4296-831d-88b65529807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BELOW WE WILL DEFINE THE COLUMNS FROM THE DATA SET \n",
    "## AND CREATE FIT_PREPROCESSING\n",
    "target_col = \"Transported\"\n",
    "drop_cols = [\"PassengerId\", \"Name\", \"Cabin\"]\n",
    "numeric_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"TotalSpend\", \"NoSpend\", \"CabinNum\", \"GroupSize\", \"IsAlone\"]\n",
    "categorical_cols = [\"HomePlanet\", \"CryoSleep\", \"CabinDeck\", \"CabinSide\", \"Destination\", \"VIP\"]\n",
    "\n",
    "def fit_preprocessing (df_train):\n",
    "    y = df_train[target_col].astype(int)\n",
    "    X = df_train.drop(columns=drop_cols + [target_col])\n",
    "\n",
    "    X_numeric = X[numeric_cols].copy()\n",
    "    X_cat = X[categorical_cols].copy()\n",
    "\n",
    "    numeric_medians = X_numeric.median()\n",
    "    cat_modes = X_cat.mode().iloc[0]\n",
    "    \n",
    "    ## add in median for NaNs on numerical cols\n",
    "    for col in numeric_cols:\n",
    "        X_numeric[col] = X_numeric[col].fillna(numeric_medians[col])\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        X_cat[col] = X_cat[col].fillna(cat_modes[col])\n",
    "\n",
    "    if X_numeric.isna().sum().sum() != 0:\n",
    "        print(\"Error: not null values in numeric cols still unfilled\")\n",
    "        return\n",
    "\n",
    "    if X_cat.isna().sum().sum() != 0:\n",
    "        print(\"Error: not null values still in cat. calls still unfilled\")\n",
    "        return\n",
    "\n",
    "    X_cat_encoded = pd.get_dummies(X_cat, drop_first=False)  \n",
    "    \n",
    "    X_prepared = pd.concat([X_numeric, X_cat_encoded], axis=1)\n",
    "    print(X_prepared.shape)\n",
    "\n",
    "    return X_prepared, numeric_medians, cat_modes, y, X_prepared.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8d97b3-f25a-40be-a0e6-45beceff139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(df, numeric_medians, cat_modes, train_cols):\n",
    "    X = df.drop(columns=drop_cols)\n",
    "\n",
    "    X_numeric = X[numeric_cols].copy()\n",
    "    X_cat = X[categorical_cols].copy()\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        X_numeric[col] = X_numeric[col].fillna(numeric_medians[col])\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        X_cat[col] = X_cat[col].fillna(cat_modes[col])\n",
    "\n",
    "    if X_numeric.isna().sum().sum() != 0:\n",
    "        print(\"Error: null values in numeric cols still unfilled\")\n",
    "        return\n",
    "\n",
    "    if X_cat.isna().sum().sum() != 0:\n",
    "        print(\"Error: null values still in cat. calls still unfilled\")\n",
    "        return\n",
    "\n",
    "    X_cat_encoded = pd.get_dummies(X_cat, drop_first=False)\n",
    "\n",
    "    X_prepared = pd.concat([X_numeric, X_cat_encoded], axis=1)\n",
    "\n",
    "    X_prepared = X_prepared.reindex(columns=train_cols, fill_value=0)\n",
    "\n",
    "    return X_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a66f57-c4a6-4cae-b5f9-63b2f1713e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8693, 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/q3ls96hn7q7c5f2kxhnz3pf00000gn/T/ipykernel_50355/4281073105.py:23: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_cat[col] = X_cat[col].fillna(cat_modes[col])\n",
      "/var/folders/0q/q3ls96hn7q7c5f2kxhnz3pf00000gn/T/ipykernel_50355/3688511642.py:11: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_cat[col] = X_cat[col].fillna(cat_modes[col])\n"
     ]
    }
   ],
   "source": [
    "X_prepared, numeric_medians, cat_modes, y, train_cols = fit_preprocessing(df_train)\n",
    "X_test_prepared = apply_preprocessing(df_test, numeric_medians, cat_modes, train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2269e1-9c6c-4d91-8e42-0e36c67396a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_prepared, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a27fd4b-e666-4245-bdcb-83eda48abcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr=0.40 -> mean_acc=0.80916\n",
      "thr=0.41 -> mean_acc=0.80996\n",
      "thr=0.42 -> mean_acc=0.81077\n",
      "thr=0.43 -> mean_acc=0.81111\n",
      "thr=0.44 -> mean_acc=0.81042\n",
      "thr=0.45 -> mean_acc=0.81054\n",
      "thr=0.46 -> mean_acc=0.81123\n",
      "thr=0.47 -> mean_acc=0.81123\n",
      "thr=0.48 -> mean_acc=0.81134\n",
      "thr=0.49 -> mean_acc=0.81065\n",
      "thr=0.50 -> mean_acc=0.80996\n",
      "thr=0.51 -> mean_acc=0.80766\n",
      "thr=0.52 -> mean_acc=0.80697\n",
      "thr=0.53 -> mean_acc=0.80789\n",
      "thr=0.54 -> mean_acc=0.80720\n",
      "thr=0.55 -> mean_acc=0.80525\n",
      "thr=0.56 -> mean_acc=0.80548\n",
      "thr=0.57 -> mean_acc=0.80617\n",
      "thr=0.58 -> mean_acc=0.80674\n",
      "thr=0.59 -> mean_acc=0.80582\n",
      "thr=0.60 -> mean_acc=0.80364\n",
      "Best threshold: 0.48 with CV acc: 0.8113416503936298\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "groups = df_train[\"GroupId\"]\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "thresholds = np.linspace(0.4, 0.6, 21)  # 0.40, 0.41, ..., 0.60\n",
    "\n",
    "best_thr = None\n",
    "best_acc = -1\n",
    "\n",
    "for thr in thresholds:\n",
    "    fold_accs = []\n",
    "\n",
    "    for train_idx, val_idx in gkf.split(X_prepared, y, groups=groups):\n",
    "        X_tr, X_va = X_prepared.iloc[train_idx], X_prepared.iloc[val_idx]\n",
    "        y_tr, y_va = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        gb = HistGradientBoostingClassifier(\n",
    "            learning_rate=0.05,\n",
    "            max_depth=None,\n",
    "            max_iter=300,\n",
    "            min_samples_leaf=20,\n",
    "            random_state=42,\n",
    "        )\n",
    "        gb.fit(X_tr, y_tr)\n",
    "        proba_va = gb.predict_proba(X_va)[:, 1]\n",
    "\n",
    "        preds_va = (proba_va >= thr).astype(int)\n",
    "        acc = accuracy_score(y_va, preds_va)\n",
    "        fold_accs.append(acc)\n",
    "\n",
    "    mean_acc = np.mean(fold_accs)\n",
    "    print(f\"thr={thr:.2f} -> mean_acc={mean_acc:.5f}\")\n",
    "\n",
    "    if mean_acc > best_acc:\n",
    "        best_acc = mean_acc\n",
    "        best_thr = thr\n",
    "\n",
    "print(\"Best threshold:\", best_thr, \"with CV acc:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1455aad-151e-48a1-a5be-a6b3a977ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. Train final HGB on all training data\n",
    "gb_final = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=None,\n",
    "    max_iter=300,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    ")\n",
    "gb_final.fit(X_prepared, y)\n",
    "\n",
    "# 2. Predict probabilities on test\n",
    "proba_test = gb_final.predict_proba(X_test_prepared)[:, 1]\n",
    "\n",
    "# 3. Use tuned threshold instead of 0.5\n",
    "best_thr = 0.48  # <-- replace with the value you found from the sweep\n",
    "pred_test = (proba_test >= best_thr).astype(bool)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": df_test[\"PassengerId\"],\n",
    "    \"Transported\": pred_test,\n",
    "})\n",
    "\n",
    "# os.makedirs(\"submissions\", exist_ok=True)\n",
    "# submission.to_csv(\"submissions/06_hgb_threshold_tuned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b0068c-8f07-40e5-bb5d-79402539a831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGB   CV mean: 0.8113416503936298\n",
      "XGB   CV mean: 0.8111124272180023\n",
      "Ensem CV mean: 0.8123772574082297\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "groups = df_train[\"GroupId\"]\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "acc_hgb = []\n",
    "acc_xgb = []\n",
    "acc_ens = []\n",
    "\n",
    "for train_idx, val_idx in gkf.split(X_prepared, y, groups=groups):\n",
    "    X_tr, X_va = X_prepared.iloc[train_idx], X_prepared.iloc[val_idx]\n",
    "    y_tr, y_va = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # HGB (your best config)\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=None,\n",
    "        max_iter=300,\n",
    "        min_samples_leaf=20,\n",
    "        random_state=42,\n",
    "    )\n",
    "    hgb.fit(X_tr, y_tr)\n",
    "    proba_hgb = hgb.predict_proba(X_va)[:, 1]\n",
    "\n",
    "    \n",
    "\n",
    "    # XGBoost (decent starting point)\n",
    "\n",
    "# Best config:\n",
    "# learning_rate         0.050000\n",
    "# n_estimators        200.000000\n",
    "# max_depth             5.000000\n",
    "# subsample             0.800000\n",
    "# colsample_bytree      1.000000\n",
    "# cv_mean_acc           0.812608\n",
    "# cv_std_acc            0.005153\n",
    "# Name: 5, dtype: float64\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=1.0,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "    xgb.fit(X_tr, y_tr)\n",
    "    proba_xgb = xgb.predict_proba(X_va)[:, 1]\n",
    "\n",
    "    # Threshold 0.48 to match your tuned best\n",
    "    thr = 0.48\n",
    "\n",
    "    pred_hgb = (proba_hgb >= thr).astype(int)\n",
    "    pred_xgb = (proba_xgb >= thr).astype(int)\n",
    "    proba_ens = 0.5 * proba_hgb + 0.5 * proba_xgb\n",
    "    pred_ens = (proba_ens >= thr).astype(int)\n",
    "\n",
    "    acc_hgb.append(accuracy_score(y_va, pred_hgb))\n",
    "    acc_xgb.append(accuracy_score(y_va, pred_xgb))\n",
    "    acc_ens.append(accuracy_score(y_va, pred_ens))\n",
    "\n",
    "print(\"HGB   CV mean:\", np.mean(acc_hgb))\n",
    "print(\"XGB   CV mean:\", np.mean(acc_xgb))\n",
    "print(\"Ensem CV mean:\", np.mean(acc_ens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0778b9f-4de8-410d-9826-cbe51fc76357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.05, n_est=200, depth=4, subsample=0.8, colsample=0.8 -> mean_acc=0.80835 (std=0.0078)\n",
      "lr=0.05, n_est=200, depth=4, subsample=0.8, colsample=1.0 -> mean_acc=0.80732 (std=0.0082)\n",
      "lr=0.05, n_est=200, depth=4, subsample=1.0, colsample=0.8 -> mean_acc=0.80870 (std=0.0090)\n",
      "lr=0.05, n_est=200, depth=4, subsample=1.0, colsample=1.0 -> mean_acc=0.80847 (std=0.0096)\n",
      "lr=0.05, n_est=200, depth=5, subsample=0.8, colsample=0.8 -> mean_acc=0.80847 (std=0.0084)\n",
      "lr=0.05, n_est=200, depth=5, subsample=0.8, colsample=1.0 -> mean_acc=0.81261 (std=0.0052)\n",
      "lr=0.05, n_est=200, depth=5, subsample=1.0, colsample=0.8 -> mean_acc=0.80720 (std=0.0055)\n",
      "lr=0.05, n_est=200, depth=5, subsample=1.0, colsample=1.0 -> mean_acc=0.80916 (std=0.0080)\n",
      "lr=0.05, n_est=200, depth=6, subsample=0.8, colsample=0.8 -> mean_acc=0.80973 (std=0.0082)\n",
      "lr=0.05, n_est=200, depth=6, subsample=0.8, colsample=1.0 -> mean_acc=0.80973 (std=0.0057)\n",
      "lr=0.05, n_est=200, depth=6, subsample=1.0, colsample=0.8 -> mean_acc=0.80950 (std=0.0062)\n",
      "lr=0.05, n_est=200, depth=6, subsample=1.0, colsample=1.0 -> mean_acc=0.80570 (std=0.0076)\n",
      "lr=0.05, n_est=400, depth=4, subsample=0.8, colsample=0.8 -> mean_acc=0.80778 (std=0.0073)\n",
      "lr=0.05, n_est=400, depth=4, subsample=0.8, colsample=1.0 -> mean_acc=0.80996 (std=0.0061)\n",
      "lr=0.05, n_est=400, depth=4, subsample=1.0, colsample=0.8 -> mean_acc=0.81111 (std=0.0061)\n",
      "lr=0.05, n_est=400, depth=4, subsample=1.0, colsample=1.0 -> mean_acc=0.80950 (std=0.0083)\n",
      "lr=0.05, n_est=400, depth=5, subsample=0.8, colsample=0.8 -> mean_acc=0.80881 (std=0.0066)\n",
      "lr=0.05, n_est=400, depth=5, subsample=0.8, colsample=1.0 -> mean_acc=0.81123 (std=0.0062)\n",
      "lr=0.05, n_est=400, depth=5, subsample=1.0, colsample=0.8 -> mean_acc=0.80927 (std=0.0073)\n",
      "lr=0.05, n_est=400, depth=5, subsample=1.0, colsample=1.0 -> mean_acc=0.80743 (std=0.0039)\n",
      "lr=0.05, n_est=400, depth=6, subsample=0.8, colsample=0.8 -> mean_acc=0.80778 (std=0.0071)\n",
      "lr=0.05, n_est=400, depth=6, subsample=0.8, colsample=1.0 -> mean_acc=0.80824 (std=0.0059)\n",
      "lr=0.05, n_est=400, depth=6, subsample=1.0, colsample=0.8 -> mean_acc=0.80628 (std=0.0048)\n",
      "lr=0.05, n_est=400, depth=6, subsample=1.0, colsample=1.0 -> mean_acc=0.80640 (std=0.0046)\n",
      "lr=0.03, n_est=200, depth=4, subsample=0.8, colsample=0.8 -> mean_acc=0.80490 (std=0.0115)\n",
      "lr=0.03, n_est=200, depth=4, subsample=0.8, colsample=1.0 -> mean_acc=0.80317 (std=0.0104)\n",
      "lr=0.03, n_est=200, depth=4, subsample=1.0, colsample=0.8 -> mean_acc=0.80628 (std=0.0134)\n",
      "lr=0.03, n_est=200, depth=4, subsample=1.0, colsample=1.0 -> mean_acc=0.80582 (std=0.0128)\n",
      "lr=0.03, n_est=200, depth=5, subsample=0.8, colsample=0.8 -> mean_acc=0.80801 (std=0.0082)\n",
      "lr=0.03, n_est=200, depth=5, subsample=0.8, colsample=1.0 -> mean_acc=0.80824 (std=0.0094)\n",
      "lr=0.03, n_est=200, depth=5, subsample=1.0, colsample=0.8 -> mean_acc=0.80904 (std=0.0092)\n",
      "lr=0.03, n_est=200, depth=5, subsample=1.0, colsample=1.0 -> mean_acc=0.80847 (std=0.0084)\n",
      "lr=0.03, n_est=200, depth=6, subsample=0.8, colsample=0.8 -> mean_acc=0.80824 (std=0.0088)\n",
      "lr=0.03, n_est=200, depth=6, subsample=0.8, colsample=1.0 -> mean_acc=0.80927 (std=0.0067)\n",
      "lr=0.03, n_est=200, depth=6, subsample=1.0, colsample=0.8 -> mean_acc=0.80755 (std=0.0097)\n",
      "lr=0.03, n_est=200, depth=6, subsample=1.0, colsample=1.0 -> mean_acc=0.80708 (std=0.0086)\n",
      "lr=0.03, n_est=400, depth=4, subsample=0.8, colsample=0.8 -> mean_acc=0.80847 (std=0.0070)\n",
      "lr=0.03, n_est=400, depth=4, subsample=0.8, colsample=1.0 -> mean_acc=0.80766 (std=0.0075)\n",
      "lr=0.03, n_est=400, depth=4, subsample=1.0, colsample=0.8 -> mean_acc=0.80732 (std=0.0079)\n",
      "lr=0.03, n_est=400, depth=4, subsample=1.0, colsample=1.0 -> mean_acc=0.80939 (std=0.0104)\n",
      "lr=0.03, n_est=400, depth=5, subsample=0.8, colsample=0.8 -> mean_acc=0.80939 (std=0.0062)\n",
      "lr=0.03, n_est=400, depth=5, subsample=0.8, colsample=1.0 -> mean_acc=0.81146 (std=0.0054)\n",
      "lr=0.03, n_est=400, depth=5, subsample=1.0, colsample=0.8 -> mean_acc=0.80870 (std=0.0083)\n",
      "lr=0.03, n_est=400, depth=5, subsample=1.0, colsample=1.0 -> mean_acc=0.81031 (std=0.0088)\n",
      "lr=0.03, n_est=400, depth=6, subsample=0.8, colsample=0.8 -> mean_acc=0.80789 (std=0.0063)\n",
      "lr=0.03, n_est=400, depth=6, subsample=0.8, colsample=1.0 -> mean_acc=0.81019 (std=0.0075)\n",
      "lr=0.03, n_est=400, depth=6, subsample=1.0, colsample=0.8 -> mean_acc=0.80766 (std=0.0063)\n",
      "lr=0.03, n_est=400, depth=6, subsample=1.0, colsample=1.0 -> mean_acc=0.80847 (std=0.0066)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>cv_mean_acc</th>\n",
       "      <th>cv_std_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.812608</td>\n",
       "      <td>0.005153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811458</td>\n",
       "      <td>0.005394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811227</td>\n",
       "      <td>0.006204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.811112</td>\n",
       "      <td>0.006093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810306</td>\n",
       "      <td>0.008756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.810191</td>\n",
       "      <td>0.007462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809962</td>\n",
       "      <td>0.006146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809732</td>\n",
       "      <td>0.005657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.809731</td>\n",
       "      <td>0.008220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.809501</td>\n",
       "      <td>0.006196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809501</td>\n",
       "      <td>0.008332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.809386</td>\n",
       "      <td>0.006204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809385</td>\n",
       "      <td>0.010409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.809271</td>\n",
       "      <td>0.007297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809271</td>\n",
       "      <td>0.006731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809156</td>\n",
       "      <td>0.008002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.809042</td>\n",
       "      <td>0.009218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.808812</td>\n",
       "      <td>0.006598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.808696</td>\n",
       "      <td>0.008272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.808696</td>\n",
       "      <td>0.008952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.808467</td>\n",
       "      <td>0.007025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.808466</td>\n",
       "      <td>0.008354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.808466</td>\n",
       "      <td>0.008379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.808465</td>\n",
       "      <td>0.009557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.808465</td>\n",
       "      <td>0.006632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.808351</td>\n",
       "      <td>0.007790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.808236</td>\n",
       "      <td>0.005941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.808236</td>\n",
       "      <td>0.009351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.808236</td>\n",
       "      <td>0.008759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.808006</td>\n",
       "      <td>0.008203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.807891</td>\n",
       "      <td>0.006314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.807776</td>\n",
       "      <td>0.007302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.807776</td>\n",
       "      <td>0.007094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807661</td>\n",
       "      <td>0.007547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.807660</td>\n",
       "      <td>0.006284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.807545</td>\n",
       "      <td>0.009691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807431</td>\n",
       "      <td>0.003908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807316</td>\n",
       "      <td>0.008194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.03</td>\n",
       "      <td>400</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.807315</td>\n",
       "      <td>0.007882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.807201</td>\n",
       "      <td>0.005450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.807085</td>\n",
       "      <td>0.008617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.806396</td>\n",
       "      <td>0.004581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.806280</td>\n",
       "      <td>0.004826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.806280</td>\n",
       "      <td>0.013374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805820</td>\n",
       "      <td>0.012775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805705</td>\n",
       "      <td>0.007579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.804900</td>\n",
       "      <td>0.011493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.03</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.803175</td>\n",
       "      <td>0.010439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  n_estimators  max_depth  subsample  colsample_bytree  \\\n",
       "5            0.05           200          5        0.8               1.0   \n",
       "41           0.03           400          5        0.8               1.0   \n",
       "17           0.05           400          5        0.8               1.0   \n",
       "14           0.05           400          4        1.0               0.8   \n",
       "43           0.03           400          5        1.0               1.0   \n",
       "45           0.03           400          6        0.8               1.0   \n",
       "13           0.05           400          4        0.8               1.0   \n",
       "9            0.05           200          6        0.8               1.0   \n",
       "8            0.05           200          6        0.8               0.8   \n",
       "10           0.05           200          6        1.0               0.8   \n",
       "15           0.05           400          4        1.0               1.0   \n",
       "40           0.03           400          5        0.8               0.8   \n",
       "39           0.03           400          4        1.0               1.0   \n",
       "18           0.05           400          5        1.0               0.8   \n",
       "33           0.03           200          6        0.8               1.0   \n",
       "7            0.05           200          5        1.0               1.0   \n",
       "30           0.03           200          5        1.0               0.8   \n",
       "16           0.05           400          5        0.8               0.8   \n",
       "42           0.03           400          5        1.0               0.8   \n",
       "2            0.05           200          4        1.0               0.8   \n",
       "36           0.03           400          4        0.8               0.8   \n",
       "4            0.05           200          5        0.8               0.8   \n",
       "31           0.03           200          5        1.0               1.0   \n",
       "3            0.05           200          4        1.0               1.0   \n",
       "47           0.03           400          6        1.0               1.0   \n",
       "0            0.05           200          4        0.8               0.8   \n",
       "21           0.05           400          6        0.8               1.0   \n",
       "29           0.03           200          5        0.8               1.0   \n",
       "32           0.03           200          6        0.8               0.8   \n",
       "28           0.03           200          5        0.8               0.8   \n",
       "44           0.03           400          6        0.8               0.8   \n",
       "12           0.05           400          4        0.8               0.8   \n",
       "20           0.05           400          6        0.8               0.8   \n",
       "37           0.03           400          4        0.8               1.0   \n",
       "46           0.03           400          6        1.0               0.8   \n",
       "34           0.03           200          6        1.0               0.8   \n",
       "19           0.05           400          5        1.0               1.0   \n",
       "1            0.05           200          4        0.8               1.0   \n",
       "38           0.03           400          4        1.0               0.8   \n",
       "6            0.05           200          5        1.0               0.8   \n",
       "35           0.03           200          6        1.0               1.0   \n",
       "23           0.05           400          6        1.0               1.0   \n",
       "22           0.05           400          6        1.0               0.8   \n",
       "26           0.03           200          4        1.0               0.8   \n",
       "27           0.03           200          4        1.0               1.0   \n",
       "11           0.05           200          6        1.0               1.0   \n",
       "24           0.03           200          4        0.8               0.8   \n",
       "25           0.03           200          4        0.8               1.0   \n",
       "\n",
       "    cv_mean_acc  cv_std_acc  \n",
       "5      0.812608    0.005153  \n",
       "41     0.811458    0.005394  \n",
       "17     0.811227    0.006204  \n",
       "14     0.811112    0.006093  \n",
       "43     0.810306    0.008756  \n",
       "45     0.810191    0.007462  \n",
       "13     0.809962    0.006146  \n",
       "9      0.809732    0.005657  \n",
       "8      0.809731    0.008220  \n",
       "10     0.809501    0.006196  \n",
       "15     0.809501    0.008332  \n",
       "40     0.809386    0.006204  \n",
       "39     0.809385    0.010409  \n",
       "18     0.809271    0.007297  \n",
       "33     0.809271    0.006731  \n",
       "7      0.809156    0.008002  \n",
       "30     0.809042    0.009218  \n",
       "16     0.808812    0.006598  \n",
       "42     0.808696    0.008272  \n",
       "2      0.808696    0.008952  \n",
       "36     0.808467    0.007025  \n",
       "4      0.808466    0.008354  \n",
       "31     0.808466    0.008379  \n",
       "3      0.808465    0.009557  \n",
       "47     0.808465    0.006632  \n",
       "0      0.808351    0.007790  \n",
       "21     0.808236    0.005941  \n",
       "29     0.808236    0.009351  \n",
       "32     0.808236    0.008759  \n",
       "28     0.808006    0.008203  \n",
       "44     0.807891    0.006314  \n",
       "12     0.807776    0.007302  \n",
       "20     0.807776    0.007094  \n",
       "37     0.807661    0.007547  \n",
       "46     0.807660    0.006284  \n",
       "34     0.807545    0.009691  \n",
       "19     0.807431    0.003908  \n",
       "1      0.807316    0.008194  \n",
       "38     0.807315    0.007882  \n",
       "6      0.807201    0.005450  \n",
       "35     0.807085    0.008617  \n",
       "23     0.806396    0.004581  \n",
       "22     0.806280    0.004826  \n",
       "26     0.806280    0.013374  \n",
       "27     0.805820    0.012775  \n",
       "11     0.805705    0.007579  \n",
       "24     0.804900    0.011493  \n",
       "25     0.803175    0.010439  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best config:\n",
      "learning_rate         0.050000\n",
      "n_estimators        200.000000\n",
      "max_depth             5.000000\n",
      "subsample             0.800000\n",
      "colsample_bytree      1.000000\n",
      "cv_mean_acc           0.812608\n",
      "cv_std_acc            0.005153\n",
      "Name: 5, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "groups = df_train[\"GroupId\"]\n",
    "\n",
    "# Define a small-ish grid to start (you can expand later)\n",
    "learning_rates = [0.05, 0.03]\n",
    "n_estimators_list = [200, 400]\n",
    "max_depths = [4, 5, 6]\n",
    "subsamples = [0.8, 1.0]\n",
    "colsample_bytrees = [0.8, 1.0]\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for n_est in n_estimators_list:\n",
    "        for depth in max_depths:\n",
    "            for subs in subsamples:\n",
    "                for colsample in colsample_bytrees:\n",
    "                    fold_accs = []\n",
    "\n",
    "                    for train_idx, val_idx in gkf.split(X_prepared, y, groups=groups):\n",
    "                        X_tr, X_va = X_prepared.iloc[train_idx], X_prepared.iloc[val_idx]\n",
    "                        y_tr, y_va = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "                        xgb = XGBClassifier(\n",
    "                            learning_rate=lr,\n",
    "                            n_estimators=n_est,\n",
    "                            max_depth=depth,\n",
    "                            subsample=subs,\n",
    "                            colsample_bytree=colsample,\n",
    "                            objective=\"binary:logistic\",\n",
    "                            eval_metric=\"logloss\",\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42,\n",
    "                        )\n",
    "\n",
    "                        xgb.fit(X_tr, y_tr)\n",
    "                        proba_va = xgb.predict_proba(X_va)[:, 1]\n",
    "\n",
    "                        # using 0.5 threshold for search; you can retune later if needed\n",
    "                        preds_va = (proba_va >= 0.5).astype(int)\n",
    "                        acc = accuracy_score(y_va, preds_va)\n",
    "                        fold_accs.append(acc)\n",
    "\n",
    "                    mean_acc = np.mean(fold_accs)\n",
    "                    std_acc = np.std(fold_accs)\n",
    "\n",
    "                    print(\n",
    "                        f\"lr={lr}, n_est={n_est}, depth={depth}, \"\n",
    "                        f\"subsample={subs}, colsample={colsample} \"\n",
    "                        f\"-> mean_acc={mean_acc:.5f} (std={std_acc:.4f})\"\n",
    "                    )\n",
    "\n",
    "                    results.append({\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"n_estimators\": n_est,\n",
    "                        \"max_depth\": depth,\n",
    "                        \"subsample\": subs,\n",
    "                        \"colsample_bytree\": colsample,\n",
    "                        \"cv_mean_acc\": mean_acc,\n",
    "                        \"cv_std_acc\": std_acc,\n",
    "                    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.sort_values(\"cv_mean_acc\", ascending=False))\n",
    "\n",
    "best_idx = results_df[\"cv_mean_acc\"].idxmax()\n",
    "best_row = results_df.loc[best_idx]\n",
    "\n",
    "print(\"\\nBest config:\")\n",
    "print(best_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1519e0a8-d6f9-4123-98d9-18383568f993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weight w (HGB): 0.400\n",
      "Weight on XGB: 0.600\n",
      "Best threshold: 0.470\n",
      "Best OOF CV accuracy: 0.81399\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "groups = df_train[\"GroupId\"]\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "n_samples = len(X_prepared)\n",
    "oof_hgb = np.zeros(n_samples)\n",
    "oof_xgb = np.zeros(n_samples)\n",
    "oof_y   = y.values.copy()\n",
    "\n",
    "# 1) Build OOF probabilities for HGB and XGB\n",
    "for train_idx, val_idx in gkf.split(X_prepared, y, groups=groups):\n",
    "    X_tr, X_va = X_prepared.iloc[train_idx], X_prepared.iloc[val_idx]\n",
    "    y_tr, y_va = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # HGB (best config you found)\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=None,\n",
    "        max_iter=300,\n",
    "        min_samples_leaf=20,\n",
    "        random_state=42,\n",
    "    )\n",
    "    hgb.fit(X_tr, y_tr)\n",
    "    oof_hgb[val_idx] = hgb.predict_proba(X_va)[:, 1]\n",
    "\n",
    "    # XGB (best config you found)\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=1.0,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "    xgb.fit(X_tr, y_tr)\n",
    "    oof_xgb[val_idx] = xgb.predict_proba(X_va)[:, 1]\n",
    "\n",
    "# 2) Search over ensemble weights and thresholds\n",
    "weights = np.linspace(0.0, 1.0, 21)      # 0.00, 0.05, ..., 1.00\n",
    "thresholds = np.linspace(0.40, 0.60, 21) # 0.40, 0.41, ..., 0.60\n",
    "\n",
    "best_w = None\n",
    "best_thr = None\n",
    "best_acc = -1.0\n",
    "\n",
    "for w in weights:\n",
    "    ens_proba_base = w * oof_hgb + (1.0 - w) * oof_xgb  # combine once per w\n",
    "\n",
    "    for thr in thresholds:\n",
    "        preds = (ens_proba_base >= thr).astype(int)\n",
    "        acc = accuracy_score(oof_y, preds)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_w = w\n",
    "            best_thr = thr\n",
    "\n",
    "print(f\"Best weight w (HGB): {best_w:.3f}\")\n",
    "print(f\"Weight on XGB: {1 - best_w:.3f}\")\n",
    "print(f\"Best threshold: {best_thr:.3f}\")\n",
    "print(f\"Best OOF CV accuracy: {best_acc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4523062-6c47-4f79-8ce9-7228e57e80f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'submissions/07_hgb_xgb_ens_w0p4_thr047.csv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Train final HGB on all training data\n",
    "hgb_final = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_depth=None,\n",
    "    max_iter=300,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    ")\n",
    "hgb_final.fit(X_prepared, y)\n",
    "\n",
    "# 2. Train final XGB on all training data\n",
    "xgb_final = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=1.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_final.fit(X_prepared, y)\n",
    "\n",
    "# 3. Get test probabilities from both\n",
    "proba_hgb_test = hgb_final.predict_proba(X_test_prepared)[:, 1]\n",
    "proba_xgb_test = xgb_final.predict_proba(X_test_prepared)[:, 1]\n",
    "\n",
    "# 4. Ensemble with learned weights\n",
    "w = 0.4              # HGB weight\n",
    "thr = 0.47           # best threshold from search\n",
    "proba_ens_test = w * proba_hgb_test + (1 - w) * proba_xgb_test\n",
    "\n",
    "pred_ens_test = (proba_ens_test >= thr).astype(bool)\n",
    "\n",
    "# 5. Build submission\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": df_test[\"PassengerId\"],\n",
    "    \"Transported\": pred_ens_test,\n",
    "})\n",
    "\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "submission_path = \"submissions/07_hgb_xgb_ens_w0p4_thr047.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "submission_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafad4cd-1a25-4972-ac1f-bce2dbc530a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
